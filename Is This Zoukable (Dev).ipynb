{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T01:06:30.850871Z",
     "start_time": "2019-12-25T01:06:23.040123Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import spotipy\n",
    "import os, requests, time, random, json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T01:06:31.809847Z",
     "start_time": "2019-12-25T01:06:31.691496Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.obtain.spotify_metadata import generate_token, download_playlist_metadata\n",
    "\n",
    "from src.vinyl.build_datasets import extract_features\n",
    "from src.vinyl.build_datasets import build_dataset\n",
    "\n",
    "import src.vinyl.db_manager as crates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Globals](https://www.geeksforgeeks.org/global-local-variables-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T01:06:35.460931Z",
     "start_time": "2019-12-25T01:06:35.455320Z"
    }
   },
   "outputs": [],
   "source": [
    "# globals\n",
    "spotify_username = 'djconxn'\n",
    "user_id = \"spotify:user:djconxn\"\n",
    "zoukables_uri = \"spotify:playlist:79QPn32wwghlJfTImywNgV\"\n",
    "\n",
    "zouk_features_path = \"data/zoukable_spectral.npy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T01:06:37.813350Z",
     "start_time": "2019-12-25T01:06:37.806578Z"
    }
   },
   "outputs": [],
   "source": [
    "features_dict = {\n",
    "    librosa.feature.mfcc : {'n_mfcc':12},\n",
    "    librosa.feature.spectral_centroid : {},\n",
    "    librosa.feature.chroma_stft : {'n_chroma':12},\n",
    "    librosa.feature.spectral_contrast : {'n_bands':6},\n",
    "    #librosa.feature.tempogram : {'win_length':192}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "#### TODO: Design a schema for configuring Keras models to build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "#                    input_shape=input_shape,\n",
    "#                    padding='same', return_sequences=True))\n",
    "# seq.add(BatchNormalization())\n",
    "\n",
    "# seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "#                    padding='same', return_sequences=True))\n",
    "# seq.add(BatchNormalization())\n",
    "\n",
    "# seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "#                    padding='same', return_sequences=True))\n",
    "# seq.add(BatchNormalization())\n",
    "\n",
    "# seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "#                    padding='same', return_sequences=True))\n",
    "# seq.add(BatchNormalization())\n",
    "\n",
    "# seq.add(Conv3D(filters=1, kernel_size=(3, 3, 3),\n",
    "#                activation='sigmoid',\n",
    "#                padding='same', data_format='channels_last'))\n",
    "\n",
    "# seq.compile(loss='binary_crossentropy', optimizer='adadelta')\n",
    "\n",
    "# Keras optimizer defaults:\n",
    "# Adam   : lr=0.001, beta_1=0.9,  beta_2=0.999, epsilon=1e-8, decay=0.\n",
    "# RMSprop: lr=0.001, rho=0.9,                   epsilon=1e-8, decay=0.\n",
    "# SGD    : lr=0.01,  momentum=0.,                             decay=0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain Data\n",
    "\n",
    "Set up the Spotify client, download metadata from a Zouk playlist and a non-Zouk playlist.\n",
    "\n",
    "Download song mp3 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate Spotify Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T01:06:42.253591Z",
     "start_time": "2019-12-25T01:06:42.040613Z"
    }
   },
   "outputs": [],
   "source": [
    "token=generate_token(username=spotify_username)\n",
    "sp = spotipy.Spotify(auth=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Zouk Playlist Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T01:06:49.333404Z",
     "start_time": "2019-12-25T01:06:47.552361Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zouk_songs = crates.download_playlist_songs(sp, user_id, \"zoukables\", zoukables_uri)\n",
    "# zouk_metadata = download_playlist_metadata(user_id, zoukables_uri, \"pname\", sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Zouk Playlist Sample mp3's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-22T18:03:15.001345Z",
     "start_time": "2019-12-22T18:03:14.298300Z"
    }
   },
   "outputs": [],
   "source": [
    "# zouk_songs = crates.get_playlist_songs('zoukables')\n",
    "for song_id in zouk_songs:\n",
    "    crates.get_preview_mp3(song_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Non-Zouk Songs\n",
    "#### TODO: Remove songs in `zoukables` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-22T18:03:22.249430Z",
     "start_time": "2019-12-22T18:03:15.005866Z"
    }
   },
   "outputs": [],
   "source": [
    "non_zouk_songs = crates.sample_other_songs(n_songs=len(zouk_songs), skip_genres=[\"zoukables\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Audio Features for Songs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample 10 other genres. Add the songs from their playlists to one list. Sample `n_zouk_songs` from that list. Use these as negative cases for training our zouk classifier. Train to convergence, then repeat with another sample of non-zouk songs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Audio Features\n",
    "\n",
    "#### TODO: save features to `/data/librosa_features`\n",
    "Saving the feature array to a numpy file is a terrible caching practice.\n",
    "\n",
    "New workflow for `build_dataset`:\n",
    "- Download preview mp3's, extract features and save to `/data/librosa_features`\n",
    "- Return list of (unique) mp3's successfully downloaded + extracted\n",
    "- Add new songs if needed for balanced training sets\n",
    "- Build training dataset from already-extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-22T18:26:57.144638Z",
     "start_time": "2019-12-22T18:03:22.252581Z"
    }
   },
   "outputs": [],
   "source": [
    "zouk_data = build_dataset(zouk_songs, features_dict)\n",
    "non_zouk_data = build_dataset(non_zouk_songs, features_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-22T18:26:57.192959Z",
     "start_time": "2019-12-22T18:26:57.151749Z"
    }
   },
   "outputs": [],
   "source": [
    "target = np.array([1] * len(zouk_songs) + [0] * len(non_zouk_songs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-22T18:26:57.214203Z",
     "start_time": "2019-12-22T18:26:57.197442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(764, 1294, 32)\n",
      "(764, 1294, 32)\n"
     ]
    }
   ],
   "source": [
    "print(zouk_data.shape)\n",
    "print(non_zouk_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-22T18:26:59.774168Z",
     "start_time": "2019-12-22T18:26:57.228093Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((zouk_data, non_zouk_data))\n",
    "\n",
    "train_idx, test_idx, y_train, y_test = train_test_split(\n",
    "    range(X.shape[0]), target, test_size=0.33, random_state=42, stratify=target)\n",
    "\n",
    "X_train = X[train_idx,:,:]\n",
    "X_test = X[test_idx,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Sequences for an LSTM Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "\n",
    "#### TODO: Study Convolutional LSTMs\n",
    "I think this would make the model robust to handling similar songs in different keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-22T18:27:02.289342Z",
     "start_time": "2019-12-22T18:26:59.779398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM model ...\n",
      "WARNING:tensorflow:From /Users/cliffclive/miniconda3/envs/vinyl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/cliffclive/miniconda3/envs/vinyl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/cliffclive/miniconda3/envs/vinyl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/cliffclive/miniconda3/envs/vinyl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/cliffclive/miniconda3/envs/vinyl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Compiling ...\n",
      "WARNING:tensorflow:From /Users/cliffclive/miniconda3/envs/vinyl/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/cliffclive/miniconda3/envs/vinyl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/cliffclive/miniconda3/envs/vinyl/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1294, 128)         82432     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 1294, 64)          49408     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 144,289\n",
      "Trainable params: 144,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "print(\"Build LSTM model ...\")\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units=128, dropout=0.05, recurrent_dropout=0.35, return_sequences=True, input_shape=input_shape))\n",
    "model.add(LSTM(units=64, dropout=0.05, recurrent_dropout=0.35, return_sequences=True))\n",
    "model.add(LSTM(units=32,  dropout=0.05, recurrent_dropout=0.35, return_sequences=False))\n",
    "model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "print(\"Compiling ...\")\n",
    "opt = Adam()\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "#### TODO: log the training reports to keep track of learning rates and training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-22T18:03:02.918Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Train on 767 samples, validate on 256 samples\n",
      "Epoch 1/400\n",
      "767/767 [==============================] - 262s 341ms/step - loss: 0.6681 - acc: 0.5737 - val_loss: 0.6204 - val_acc: 0.6602\n",
      "Epoch 2/400\n",
      "767/767 [==============================] - 243s 317ms/step - loss: 0.6135 - acc: 0.6323 - val_loss: 0.5807 - val_acc: 0.6953\n",
      "Epoch 3/400\n",
      "767/767 [==============================] - 274s 357ms/step - loss: 0.6006 - acc: 0.6454 - val_loss: 0.5707 - val_acc: 0.7070\n",
      "Epoch 4/400\n",
      "767/767 [==============================] - 272s 355ms/step - loss: 0.5910 - acc: 0.6714 - val_loss: 0.5519 - val_acc: 0.7109\n",
      "Epoch 5/400\n",
      "767/767 [==============================] - 274s 357ms/step - loss: 0.5808 - acc: 0.6649 - val_loss: 0.5517 - val_acc: 0.6953\n",
      "Epoch 6/400\n",
      "767/767 [==============================] - 265s 346ms/step - loss: 0.5528 - acc: 0.6832 - val_loss: 0.5382 - val_acc: 0.7070\n",
      "Epoch 7/400\n",
      "767/767 [==============================] - 276s 360ms/step - loss: 0.5435 - acc: 0.7040 - val_loss: 0.5161 - val_acc: 0.7188\n",
      "Epoch 8/400\n",
      "767/767 [==============================] - 266s 347ms/step - loss: 0.5314 - acc: 0.7171 - val_loss: 0.5314 - val_acc: 0.7305\n",
      "Epoch 9/400\n",
      "767/767 [==============================] - 267s 348ms/step - loss: 0.5334 - acc: 0.7145 - val_loss: 0.5048 - val_acc: 0.7383\n",
      "Epoch 10/400\n",
      "767/767 [==============================] - 274s 357ms/step - loss: 0.5339 - acc: 0.7405 - val_loss: 0.5288 - val_acc: 0.7148\n",
      "Epoch 11/400\n",
      "767/767 [==============================] - 265s 346ms/step - loss: 0.5399 - acc: 0.7106 - val_loss: 0.5384 - val_acc: 0.6953\n",
      "Epoch 12/400\n",
      "767/767 [==============================] - 268s 350ms/step - loss: 0.5210 - acc: 0.7314 - val_loss: 0.5232 - val_acc: 0.7148\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 13/400\n",
      "767/767 [==============================] - 230s 300ms/step - loss: 0.5102 - acc: 0.7353 - val_loss: 0.5046 - val_acc: 0.7305\n",
      "Epoch 14/400\n",
      "767/767 [==============================] - 227s 296ms/step - loss: 0.5141 - acc: 0.7184 - val_loss: 0.4970 - val_acc: 0.7578\n",
      "Epoch 15/400\n",
      "767/767 [==============================] - 229s 298ms/step - loss: 0.5023 - acc: 0.7366 - val_loss: 0.4958 - val_acc: 0.7539\n",
      "Epoch 16/400\n",
      "767/767 [==============================] - 230s 300ms/step - loss: 0.5011 - acc: 0.7275 - val_loss: 0.4933 - val_acc: 0.7656\n",
      "Epoch 17/400\n",
      "767/767 [==============================] - 226s 295ms/step - loss: 0.4789 - acc: 0.7575 - val_loss: 0.4970 - val_acc: 0.7500\n",
      "Epoch 18/400\n",
      "767/767 [==============================] - 223s 291ms/step - loss: 0.4674 - acc: 0.7705 - val_loss: 0.5197 - val_acc: 0.7188\n",
      "Epoch 19/400\n",
      "767/767 [==============================] - 224s 291ms/step - loss: 0.4863 - acc: 0.7601 - val_loss: 0.5173 - val_acc: 0.7266\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 20/400\n",
      "767/767 [==============================] - 222s 290ms/step - loss: 0.4980 - acc: 0.7523 - val_loss: 0.4807 - val_acc: 0.7539\n",
      "Epoch 21/400\n",
      "767/767 [==============================] - 226s 295ms/step - loss: 0.4792 - acc: 0.7575 - val_loss: 0.4731 - val_acc: 0.7500\n",
      "Epoch 22/400\n",
      "767/767 [==============================] - 233s 303ms/step - loss: 0.4577 - acc: 0.7797 - val_loss: 0.4720 - val_acc: 0.7617\n",
      "Epoch 23/400\n",
      "767/767 [==============================] - 225s 293ms/step - loss: 0.4782 - acc: 0.7640 - val_loss: 0.4782 - val_acc: 0.7539\n",
      "Epoch 24/400\n",
      "767/767 [==============================] - 232s 303ms/step - loss: 0.4649 - acc: 0.7601 - val_loss: 0.4738 - val_acc: 0.7500\n",
      "Epoch 25/400\n",
      "767/767 [==============================] - 223s 291ms/step - loss: 0.4532 - acc: 0.7718 - val_loss: 0.4678 - val_acc: 0.7539\n",
      "Epoch 26/400\n",
      "767/767 [==============================] - 263s 342ms/step - loss: 0.4718 - acc: 0.7614 - val_loss: 0.4813 - val_acc: 0.7578\n",
      "Epoch 27/400\n",
      "767/767 [==============================] - 226s 294ms/step - loss: 0.4870 - acc: 0.7614 - val_loss: 0.4795 - val_acc: 0.7305\n",
      "Epoch 28/400\n",
      "767/767 [==============================] - 248s 323ms/step - loss: 0.4638 - acc: 0.7549 - val_loss: 0.4781 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 29/400\n",
      "767/767 [==============================] - 225s 293ms/step - loss: 0.4824 - acc: 0.7484 - val_loss: 0.4764 - val_acc: 0.7461\n",
      "Epoch 30/400\n",
      "767/767 [==============================] - 224s 292ms/step - loss: 0.4806 - acc: 0.7588 - val_loss: 0.4681 - val_acc: 0.7578\n",
      "Epoch 31/400\n",
      "767/767 [==============================] - 228s 298ms/step - loss: 0.4502 - acc: 0.7718 - val_loss: 0.4677 - val_acc: 0.7656\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 32/400\n",
      "767/767 [==============================] - 263s 343ms/step - loss: 0.4475 - acc: 0.7849 - val_loss: 0.4687 - val_acc: 0.7617\n",
      "Epoch 33/400\n",
      "767/767 [==============================] - 226s 294ms/step - loss: 0.4396 - acc: 0.7888 - val_loss: 0.4712 - val_acc: 0.7422\n",
      "Epoch 34/400\n",
      "767/767 [==============================] - 226s 294ms/step - loss: 0.4549 - acc: 0.7653 - val_loss: 0.4691 - val_acc: 0.7578\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 35/400\n",
      "767/767 [==============================] - 226s 295ms/step - loss: 0.4533 - acc: 0.7862 - val_loss: 0.4684 - val_acc: 0.7578\n",
      "Epoch 36/400\n",
      "767/767 [==============================] - 222s 290ms/step - loss: 0.4310 - acc: 0.7927 - val_loss: 0.4688 - val_acc: 0.7617\n",
      "Epoch 37/400\n",
      "767/767 [==============================] - 224s 292ms/step - loss: 0.4515 - acc: 0.7966 - val_loss: 0.4683 - val_acc: 0.7617\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 38/400\n",
      "767/767 [==============================] - 230s 299ms/step - loss: 0.4462 - acc: 0.7862 - val_loss: 0.4677 - val_acc: 0.7617\n",
      "Epoch 39/400\n",
      "767/767 [==============================] - 222s 289ms/step - loss: 0.4439 - acc: 0.7849 - val_loss: 0.4672 - val_acc: 0.7695\n",
      "Epoch 40/400\n",
      "767/767 [==============================] - 231s 301ms/step - loss: 0.4441 - acc: 0.7810 - val_loss: 0.4668 - val_acc: 0.7734\n",
      "Epoch 41/400\n",
      "767/767 [==============================] - 224s 292ms/step - loss: 0.4541 - acc: 0.8005 - val_loss: 0.4663 - val_acc: 0.7695\n",
      "Epoch 42/400\n",
      "280/767 [=========>....................] - ETA: 2:11 - loss: 0.3986 - acc: 0.8250"
     ]
    }
   ],
   "source": [
    "print(\"Training ...\")\n",
    "batch_size = 35  # num of training examples per minibatch\n",
    "num_epochs = 400\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs, \n",
    "    validation_split=.25, \n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=8, verbose=1, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=.5, patience=3, verbose=1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-22T18:03:02.921Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nTesting ...\")\n",
    "score, accuracy = model.evaluate(\n",
    "    X_test, y_test, batch_size=batch_size, verbose=1\n",
    ")\n",
    "print(\"Test loss:  \", score)\n",
    "print(\"Test accuracy:  \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-22T18:03:02.924Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"models/zouk_classifier_spectral_LSTM3.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is It Any Good?\n",
    "\n",
    "Do some explanatory analysis to see what songs are being misclassified. I know that the \"labels\" are sketchy, so I'll need to do some data cleaning and re-training. How bad is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Predictions From Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-22T18:03:02.927Z"
    }
   },
   "outputs": [],
   "source": [
    "all_songs = pd.DataFrame({'song_id':zouk_songs + non_zouk_songs,\n",
    "                          'target':target})\n",
    "\n",
    "trainers = all_songs.iloc[train_idx,:].reset_index()\n",
    "\n",
    "sample0 = trainers[trainers.target==0].sample(10).index\n",
    "sample1 = trainers[trainers.target==1].sample(10).index\n",
    "sample_idx = sample0.append(sample1)\n",
    "samples = trainers.loc[sample_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-22T18:03:02.929Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_train[sample_idx,:])\n",
    "y_pred_bool = y_pred > 0.75\n",
    "samples['prediction'] = y_pred_bool.astype(int)\n",
    "print(classification_report(samples.target, y_pred_bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Add False Positives, False Negatives to Spotify playlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_uri = 'spotify:playlist:69K5ogTF87NeSFvU9ePI3x'\n",
    "suspects_uri = 'spotify:playlist:3M1IBVChmAYh7srqwK0CDt'\n",
    "\n",
    "def update_screening_playlists(false_positives, false_negatives):\n",
    "    global user_id\n",
    "    global candidates_uri\n",
    "    global suspects_uri\n",
    "    sp.user_playlist_add_tracks(user_id, candidates_uri, false_positives)\n",
    "    sp.user_playlist_add_tracks(user_id, suspects_uri, false_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample False Positives and False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-22T18:03:02.932Z"
    }
   },
   "outputs": [],
   "source": [
    "fp_index = samples[(samples.target==0) & (samples.prediction==1)].index\n",
    "fn_index = samples[(samples.target==1) & (samples.prediction==0)].index\n",
    "\n",
    "print(\"False Positives:\")\n",
    "for i in fp_index:\n",
    "    song_id = samples['song_id'][i]\n",
    "    filepath = crates.get_preview_mp3(song_id)\n",
    "    print(crates.load_song_metadata(song_id)['title'])\n",
    "    ipd.display(ipd.Audio(filepath))\n",
    "\n",
    "print(\"~\" * 32)\n",
    "\n",
    "print(\"False Negatives:\")\n",
    "for i in fn_index:\n",
    "    song_id = samples['song_id'][i]\n",
    "    filepath = crates.get_preview_mp3(song_id)\n",
    "    print(crates.load_song_metadata(song_id)['title'])\n",
    "    ipd.display(ipd.Audio(filepath))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ship It!\n",
    "\n",
    "Create a new notebook and copy over the code it needs to run the app from scratch.\n",
    "\n",
    "Copy over the functions that return the output, and then iterate running the function and copying over the imports and function definitions that are needed to get it to execute without crashing.\n",
    "\n",
    "(MVP for this should probably run on a single song, not all the songs on a playlist... downloading and extracting the features for many songs is going to take a long time.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [Every Noise At Once](http://everynoise.com/)\n",
    "- [Keras docs](https://keras.io/)\n",
    "- [Librosa docs](https://librosa.github.io/librosa/index.html)\n",
    "- [Spotipy docs](https://spotipy.readthedocs.io)\n",
    "- [ruohoruotsi: LSTM Music Genre Classification on GitHub](https://github.com/ruohoruotsi/LSTM-Music-Genre-Classification)\n",
    "- [Music Genre classification using a hierarchical Long Short Term Memory (LSTM) Model](http://www.cs.cuhk.hk/~khwong/p186_acm_00_main_lstm_music_rev5.pdf)\n",
    "- [Using CNNs and RNNs for Music Genre Recognition](https://towardsdatascience.com/using-cnns-and-rnns-for-music-genre-recognition-2435fb2ed6af) [(GitHub)](https://github.com/priya-dwivedi/Music_Genre_Classification)\n",
    "- [The dummyâ€™s guide to MFCC](https://medium.com/prathena/the-dummys-guide-to-mfcc-aceab2450fd)\n",
    "- [Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting](https://arxiv.org/abs/1506.04214v1)\n",
    "- [An introduction to ConvLSTM](https://medium.com/neuronio/an-introduction-to-convlstm-55c9025563a7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage Space Requirements\n",
    "\n",
    ".model files = 1 - 6 MB\n",
    "\n",
    "features = 250MB (spectral), 1.7GB(tempo)\n",
    "\n",
    "mp3 previews = 365 kB ea\n",
    "\n",
    "librosa features = 420 kB ea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Plan\n",
    "\n",
    "This process should train a decent classifier for songs from this playlist, but I really need to find a much larger list of positive cases. My plan is to maintain three playlists on Spotify: \n",
    "- the Zoukables list, which I've curated\n",
    "- a False Positives list, non-zouk songs which have been classified as zoukable and may very well be zoukable (since in our workflow, \"negative\" just means \"has not been tagged positive\"), which I can then screen and possibly add to the Zoukables list\n",
    "- a False Negatives list, zouk songs which have been classified as not zoukable and may not actually belong in the Zoukables list\n",
    "\n",
    "Once I set up these playlists and connect them to my pipeline, I can run and re-run the training pipeline, and listen and screen the Spotify playlists to curate my training set.\n",
    "\n",
    "And then the next step would be to engineer a system where other users can vote on songs to add to the Zoukables list, and automatically add songs with a threshold of votes and a high enough percentage of Yes votes.\n",
    "\n",
    "## Spotify Playlist Updates\n",
    "\n",
    "- Refresh Zoukables list when training models\n",
    "- Update FP/FN screening playlists on Spotify\n",
    "- Update GitHub\n",
    "\n",
    "## Re-implement EveryNoise Scraper\n",
    "\n",
    "(I think this is working)\n",
    "\n",
    "- Download EveryNoise playlist URLs\n",
    "- Download Spotify playlist metadata\n",
    "- Download preview mp3s (during model training)\n",
    "- Update GitHub\n",
    "\n",
    "## Mongo DB: Songs Database\n",
    "\n",
    "(I've got this working in flat files)\n",
    "\n",
    "- Song IDs\n",
    "- Spotify metadata\n",
    "- Librosa Features\n",
    "- Genre Labels\n",
    "- Python API (1.4.0.1/2/3, 2.1.0.1)\n",
    "- Update GitHub\n",
    "\n",
    "## Mongo DB: Models Database\n",
    "\n",
    "- Keras schema (0.3.2.1)\n",
    "- Feature sets\n",
    "- Training reports\n",
    "- Python API (3.1.0.1, 3.2.0.1)\n",
    "- Update GitHub\n",
    "\n",
    "## Python Package\n",
    "\n",
    "- Keras model API\n",
    "- Organize modules\n",
    "- Write docstrings\n",
    "- Conda environment\n",
    "- Update GitHub\n",
    "\n",
    "## Deployment\n",
    "- Reproduce pipeline on other machines\n",
    "- Reproduce pipeline for other genres\n",
    "- Deploy to AWS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vinyl",
   "language": "python",
   "name": "vinyl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "276.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
