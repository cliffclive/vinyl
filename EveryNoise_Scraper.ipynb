{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "#### TODO: package functions\n",
    "#### TODO: clean up imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T06:05:14.223248Z",
     "start_time": "2019-12-21T06:05:14.207220Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# IMPORTS \n",
    "# TODO: move most functions into utility modules, along with their required imports\n",
    "\n",
    "import os, pickle, re, requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import spotipy\n",
    "import spotipy.util as util\n",
    "import spotipy.oauth2 as oauth2\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import functools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "\n",
    "from src.obtain.spotify_metadata import generate_token\n",
    "\n",
    "import random\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T06:04:12.163806Z",
     "start_time": "2019-12-21T06:04:12.159056Z"
    }
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T06:04:12.184213Z",
     "start_time": "2019-12-21T06:04:12.167057Z"
    },
    "code_folding": [
     2,
     30,
     57
    ]
   },
   "outputs": [],
   "source": [
    "# FUNCTIONS TO GET GENRES AND PLAYLIST LINKS FROM EVERY-NOISE-AT-ONCE\n",
    "\n",
    "def load_or_make(creator):\n",
    "    \"\"\"\n",
    "    Loads data that is pickled at filepath if filepath exists;\n",
    "    otherwise, calls creator(*args, **kwargs) to create the data \n",
    "    and pickle it at filepath.\n",
    "    Returns the data in either case.\n",
    "    \n",
    "    Inputs:\n",
    "    - filepath: path to where data is / should be stored\n",
    "    - creator: function to create data if it is not already pickled\n",
    "    - *args, **kwargs: arguments passed to creator()\n",
    "    \n",
    "    Outputs:\n",
    "    - item: the data that is stored at filepath\n",
    "    \"\"\"\n",
    "    @functools.wraps(creator)\n",
    "    def cached_creator(filepath, *args, **kwargs):\n",
    "        if os.path.isfile(filepath):\n",
    "            with open(filepath, 'rb') as pkl:\n",
    "                item = pickle.load(pkl)\n",
    "        else:\n",
    "            item = creator(*args, **kwargs)\n",
    "            with open(filepath, 'wb') as pkl:\n",
    "                pickle.dump(item, pkl)\n",
    "        return item\n",
    "    return cached_creator\n",
    "\n",
    "@load_or_make\n",
    "def scrape_all_links(domain, path, target_pattern):\n",
    "    \"\"\"\n",
    "    Scrapes a website and compiles a list of urls that match a target pattern.\n",
    "    \n",
    "    Inputs: \n",
    "    - domain: domain of the website you want to scrape\n",
    "    - path: path to the page that you want to scrape from `domain`\n",
    "    - target_pattern: regex that specifies the types of links you want to collect\n",
    "    \n",
    "    Outputs:\n",
    "    - target_urls: list of all the links on domain/path that match target_pattern\n",
    "    \"\"\"\n",
    "    main_page = '/'.join(['http:/', domain, path])\n",
    "    response = requests.get(main_page)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise ConnectionError(f\"Failed to connect to {main_page}.\")\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "    target_regex = re.compile(target_pattern)\n",
    "    target_urls = ['/'.join(['http:/', domain, x['href']])\n",
    "                    for x in soup.find_all('a', {'href':target_regex})]\n",
    "\n",
    "    return target_urls\n",
    "\n",
    "@load_or_make\n",
    "def scrape_links_from_each_page(urls, target_pattern, labeler=(lambda x:x)):\n",
    "    \"\"\"\n",
    "    Loops over a list of urls and finds links that matches a target pattern from each page.\n",
    "    \n",
    "    Inputs:\n",
    "    - urls: the list of urls to scrape links from\n",
    "    - target_pattern: regex that specifies the types of links you want to collect\n",
    "    - labeler: function that parses a url and returns a label for that page\n",
    "    \n",
    "    Outputs:\n",
    "    - links: a dictionary with key/value pairs {url_label:[scraped_links]}\n",
    "    \"\"\"\n",
    "    links = {}\n",
    "\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        label = labeler(url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise ConnectionError(f\"Failed to connect to {url}.\")\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "        target_regex = re.compile(target_pattern)\n",
    "        target_urls = [x['href'] for x in soup.find_all('a', {'href':target_regex})]\n",
    "\n",
    "        links[label] = target_urls\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T06:04:12.206562Z",
     "start_time": "2019-12-21T06:04:12.188899Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# FUNCTIONS TO GET PLAYLIST METADATA FROM SPOTIFY\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_tags(track):\n",
    "    '''\n",
    "    Parse metadata for a spotify track\n",
    "    From a user_playlist json file, a track can be found via:\n",
    "        user_playlist['tracks']['items'][i]\n",
    "    '''\n",
    "    tags =  {\n",
    "        'id': track['id'],\n",
    "        'album': track['album']['name'],\n",
    "        'track': track['track_number'],\n",
    "        'title': track['name'],\n",
    "        'artist': track['artists'][0]['name'],\n",
    "        'duration': int(track['duration_ms']/1000),\n",
    "        'preview_mp3': track['preview_url'],\n",
    "        'is_explicit': track['explicit'],\n",
    "        'isrc_number': track['external_ids'].get('isrc', ''),\n",
    "        'release_date': track['album']['release_date']\n",
    "    }\n",
    "    if track['album']['images']:\n",
    "        tags['cover_art_url'] = track['album']['images'][0]['url']\n",
    "    return tags\n",
    "\n",
    "def build_metadata_df(tracks, client):\n",
    "    metadata = [get_tags(item['track']) for item in tracks['items'] if item['track']]\n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    # add more features from the tracks' audio features JSON\n",
    "    features = client.audio_features(list(metadata_df['id']))\n",
    "    features_df = pd.DataFrame(features)\n",
    "    metadata_df = pd.merge(metadata_df, features_df)\n",
    "\n",
    "    return metadata_df\n",
    "\n",
    "def download_playlist_metadata(user, pid, pname, client):\n",
    "    # get metadata for playlist 'pname' by 'user'\n",
    "    results = client.user_playlist(user, pid, fields=\"tracks,next\")\n",
    "    tracks = results['tracks']\n",
    "\n",
    "    all_dfs = []\n",
    "    batch_df = build_metadata_df(tracks, client)\n",
    "    all_dfs.append(batch_df)\n",
    "\n",
    "    while tracks['next']:\n",
    "        tracks = client.next(tracks)\n",
    "        batch_df = build_metadata_df(tracks, client)\n",
    "        all_dfs.append(batch_df)\n",
    "    metadata = pd.concat(all_dfs)\n",
    "    metadata.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def parse_sos_pid(playlists):\n",
    "    return [x.split('/')[-1] for x in playlists if 'thesoundsofspotify' in x][0]\n",
    "\n",
    "def download_all_genres_metadata(genre_playlists, client):\n",
    "    for k, v in genre_playlists.items():\n",
    "        genre = k\n",
    "        filepath = f'data/genre_metadata/{genre}_metadata.tsv'\n",
    "        if os.path.isfile(filepath):\n",
    "            continue\n",
    "        playlist_id = parse_sos_pid(v)\n",
    "        metadata = download_playlist_metadata(user='thesoundsofspotify', \n",
    "                                              pid=playlist_id, \n",
    "                                              pname=genre, \n",
    "                                              client=client)\n",
    "        metadata.to_csv(filepath, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T06:10:20.593271Z",
     "start_time": "2019-12-21T06:10:20.587579Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def run_data_pipeline(token):\n",
    "    \"\"\"\n",
    "    - scrape genre page urls from everynoise.com/engenremap.html,\n",
    "        save as a list in ../data/raw/everynoise_genre_urls.pkl\n",
    "        \n",
    "    - scrape genre playlist urls from each genre page on everynoise.com,\n",
    "        save as a dictionary in ../data/raw/thesoundsofspotify_playlist_urls.pkl\n",
    "        \n",
    "    - download playlist metadata for each playlist from Spotify,\n",
    "        save as TSV files in ../data/raw/thesoundsofspotify/[genre].tsv\n",
    "        \n",
    "    - download audio_analysis files for each song in a list of playlists\n",
    "        (not necessarily all playlists because there are 100s of 1000s in the full set)\n",
    "        save as audio_analysis dictionaries in ../data/raw/audio_analysis/[song_uri].pkl\n",
    "    \n",
    "    TODO: include a progress indicator?\n",
    "    \"\"\"\n",
    "    genre_urls = scrape_all_links(\n",
    "        'data/everynoise/everynoise_genre_urls.pkl',\n",
    "        domain='everynoise.com', \n",
    "        index='engenremap.html', \n",
    "        target_pattern='engenremap-[a-z]*')\n",
    "    \n",
    "    genre_playlists = scrape_links_from_each_page(\n",
    "        'data/everynoise/thesoundsofspotify_playlist_urls.pkl',\n",
    "        urls=genre_urls,\n",
    "        labeler=(lambda url: url.split('/')[-1].split('-')[-1].split('.')[0]),\n",
    "        target_pattern='open.spotify.com')\n",
    "    \n",
    "    sp = spotipy.Spotify(auth=token)\n",
    "    \n",
    "    download_all_genres_metadata(genre_playlists, sp)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T19:57:23.759275Z",
     "start_time": "2019-06-04T19:50:53.821423Z"
    }
   },
   "outputs": [],
   "source": [
    "token=generate_token(username='djconxn')\n",
    "run_data_pipeline(token)\n",
    "\n",
    "# all genre urls from Every Noise are now saved as a list in 'data/raw/everynoise_genre_urls.pkl'\n",
    "# all genres' playlist links are saved in a dictionary in 'data/raw/thesoundsofspotify_playlist_urls.pkl'\n",
    "# each genre's playlist metadata is saved in 'data/interim/genre_metadata/{genre}_metadata.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:11:30.273764Z",
     "start_time": "2019-07-24T19:11:30.226299Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/raw/thesoundsofspotify_playlist_urls.pkl', 'rb') as urls:\n",
    "    genre_playlists = pickle.load(urls)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Mongo Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T21:34:39.557152Z",
     "start_time": "2019-12-19T21:34:39.476595Z"
    }
   },
   "source": [
    "#### TODO: Use this data schema instead.\n",
    "\n",
    "Old Schema:\n",
    "- `data/genre_metadata/{genre}_metadata.tsv` : DataFrame of Spotify metadata for each song in genre playlist.\n",
    "\n",
    "New Schema:\n",
    "- `data/genre_playlists/{genre}.txt` : list of song_id's for each genre's playlist.\n",
    "- `data/song_metadata/{song_id}.json` : JSON file containing song_id's metadata.\n",
    "\n",
    "~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ \n",
    "\n",
    "Each song's sample mp3 may be found in `/data/mp3s/{song_id}.mp3`.\n",
    "\n",
    "Each song's Spotify metadata may be found in `/data/song_metadata/{song_id}.json`. \n",
    "\n",
    "Note that not all mp3s have been downloaded.\n",
    "\n",
    "There are over 500K songs across the 2900 Sounds Of Spotify genre playlists (and these are updated, which needs to be built into this pipeline). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp3s_path = \"data/mp3s\"\n",
    "metadata_path = 'data/genre_metadata'\n",
    "song_md_path = 'data/song_metadata'\n",
    "genre_lists_path = 'data/genre_playlists'\n",
    "\n",
    "metadata_files = os.listdir(metadata_path)\n",
    "\n",
    "for md_file in metadata_files:\n",
    "\n",
    "    try:\n",
    "        meta_df = pd.read_csv(os.path.join(metadata_path, md_file), sep='\\t')\n",
    "    except:\n",
    "        print(md_file)\n",
    "        continue\n",
    "    \n",
    "    # Write playlist song id's in a text file\n",
    "    genre_name = md_file.replace('_metadata.tsv', '')\n",
    "    genre_list_path = os.path.join(genre_lists_path, genre_name) + '.txt'\n",
    "    if not os.path.isfile(genre_list_path):\n",
    "        with open(genre_list_path, 'w') as file:\n",
    "            file.writelines('\\n'.join(list(meta_df.id)))\n",
    "\n",
    "    # Write each song's metadata to a json file\n",
    "    for i in meta_df.index:\n",
    "        json_file = \"{}.json\".format(meta_df.loc[i]['id'])\n",
    "        json_path = os.path.join(song_md_path, json_file)\n",
    "        if not os.path.isfile(json_path):\n",
    "            meta_df.loc[i].to_json(json_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vinyl",
   "language": "python",
   "name": "vinyl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "249.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
