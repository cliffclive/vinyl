{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:32:53.412900Z",
     "start_time": "2019-12-18T18:32:47.884655Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# IMPORTS \n",
    "# TODO: move most functions into utility modules, along with their required imports\n",
    "\n",
    "import os, pickle, re, requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import spotipy\n",
    "import spotipy.util as util\n",
    "import spotipy.oauth2 as oauth2\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import functools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "\n",
    "import random\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     2,
     30,
     57
    ]
   },
   "outputs": [],
   "source": [
    "# FUNCTIONS TO GET GENRES AND PLAYLIST LINKS FROM EVERY-NOISE-AT-ONCE\n",
    "\n",
    "def load_or_make(creator):\n",
    "    \"\"\"\n",
    "    Loads data that is pickled at filepath if filepath exists;\n",
    "    otherwise, calls creator(*args, **kwargs) to create the data \n",
    "    and pickle it at filepath.\n",
    "    Returns the data in either case.\n",
    "    \n",
    "    Inputs:\n",
    "    - filepath: path to where data is / should be stored\n",
    "    - creator: function to create data if it is not already pickled\n",
    "    - *args, **kwargs: arguments passed to creator()\n",
    "    \n",
    "    Outputs:\n",
    "    - item: the data that is stored at filepath\n",
    "    \"\"\"\n",
    "    @functools.wraps(creator)\n",
    "    def cached_creator(filepath, *args, **kwargs):\n",
    "        if os.path.isfile(filepath):\n",
    "            with open(filepath, 'rb') as pkl:\n",
    "                item = pickle.load(pkl)\n",
    "        else:\n",
    "            item = creator(*args, **kwargs)\n",
    "            with open(filepath, 'wb') as pkl:\n",
    "                pickle.dump(item, pkl)\n",
    "        return item\n",
    "    return cached_creator\n",
    "\n",
    "@load_or_make\n",
    "def scrape_all_links(domain, path, target_pattern):\n",
    "    \"\"\"\n",
    "    Scrapes a website and compiles a list of urls that match a target pattern.\n",
    "    \n",
    "    Inputs: \n",
    "    - domain: domain of the website you want to scrape\n",
    "    - path: path to the page that you want to scrape from `domain`\n",
    "    - target_pattern: regex that specifies the types of links you want to collect\n",
    "    \n",
    "    Outputs:\n",
    "    - target_urls: list of all the links on domain/path that match target_pattern\n",
    "    \"\"\"\n",
    "    main_page = '/'.join(['http:/', domain, path])\n",
    "    response = requests.get(main_page)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise ConnectionError(f\"Failed to connect to {main_page}.\")\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "    target_regex = re.compile(target_pattern)\n",
    "    target_urls = ['/'.join(['http:/', domain, x['href']])\n",
    "                    for x in soup.find_all('a', {'href':target_regex})]\n",
    "\n",
    "    return target_urls\n",
    "\n",
    "@load_or_make\n",
    "def scrape_links_from_each_page(urls, target_pattern, labeler=(lambda x:x)):\n",
    "    \"\"\"\n",
    "    Loops over a list of urls and finds links that matches a target pattern from each page.\n",
    "    \n",
    "    Inputs:\n",
    "    - urls: the list of urls to scrape links from\n",
    "    - target_pattern: regex that specifies the types of links you want to collect\n",
    "    - labeler: function that parses a url and returns a label for that page\n",
    "    \n",
    "    Outputs:\n",
    "    - links: a dictionary with key/value pairs {url_label:[scraped_links]}\n",
    "    \"\"\"\n",
    "    links = {}\n",
    "\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        label = labeler(url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise ConnectionError(f\"Failed to connect to {url}.\")\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "        target_regex = re.compile(target_pattern)\n",
    "        target_urls = [x['href'] for x in soup.find_all('a', {'href':target_regex})]\n",
    "\n",
    "        links[label] = target_urls\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T02:39:51.129595Z",
     "start_time": "2019-06-13T02:39:51.081828Z"
    },
    "code_folding": [
     5,
     27,
     41,
     59,
     62
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/download_metadata.py\n"
     ]
    }
   ],
   "source": [
    "# FUNCTIONS TO GET PLAYLIST METADATA FROM SPOTIFY\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_tags(track):\n",
    "    '''\n",
    "    Parse metadata for a spotify track\n",
    "    From a user_playlist json file, a track can be found via:\n",
    "        user_playlist['tracks']['items'][i]\n",
    "    '''\n",
    "    tags =  {\n",
    "        'id': track['id'],\n",
    "        'album': track['album']['name'],\n",
    "        'track': track['track_number'],\n",
    "        'title': track['name'],\n",
    "        'artist': track['artists'][0]['name'],\n",
    "        'duration': int(track['duration_ms']/1000),\n",
    "        'preview_mp3': track['preview_url'],\n",
    "        'is_explicit': track['explicit'],\n",
    "        'isrc_number': track['external_ids'].get('isrc', ''),\n",
    "        'release_date': track['album']['release_date']\n",
    "    }\n",
    "    if track['album']['images']:\n",
    "        tags['cover_art_url'] = track['album']['images'][0]['url']\n",
    "    return tags\n",
    "\n",
    "def build_metadata_df(tracks, client):\n",
    "    #metadata = []\n",
    "    #for track in tracks['items']:\n",
    "    #    # read tags from the playlist JSON\n",
    "    #    metadata.append(get_tags(track['track']))\n",
    "    metadata = [get_tags(item['track']) for item in tracks['items'] if item['track']]\n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    # add more features from the tracks' audio features JSON\n",
    "    features = client.audio_features(list(metadata_df['id']))\n",
    "    features_df = pd.DataFrame(features)\n",
    "    metadata_df = pd.merge(metadata_df, features_df)\n",
    "\n",
    "    return metadata_df\n",
    "\n",
    "def download_playlist_metadata(user, pid, pname, client):\n",
    "    # get metadata for playlist 'pname' by 'user'\n",
    "    results = client.user_playlist(user, pid, fields=\"tracks,next\")\n",
    "    tracks = results['tracks']\n",
    "\n",
    "    all_dfs = []\n",
    "    batch_df = build_metadata_df(tracks, client)\n",
    "    all_dfs.append(batch_df)\n",
    "\n",
    "    while tracks['next']:\n",
    "        tracks = client.next(tracks)\n",
    "        batch_df = build_metadata_df(tracks, client)\n",
    "        all_dfs.append(batch_df)\n",
    "    metadata = pd.concat(all_dfs)\n",
    "    metadata.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def parse_sos_pid(playlists):\n",
    "    return [x.split('/')[-1] for x in playlists if 'thesoundsofspotify' in x][0]\n",
    "\n",
    "def download_all_genres_metadata(genre_playlists, client):\n",
    "    for k, v in genre_playlists.items():\n",
    "        genre = k\n",
    "        filepath = f'data/interim/genre_metadata/{genre}_metadata.tsv'\n",
    "        if os.path.isfile(filepath):\n",
    "            continue\n",
    "        playlist_id = parse_sos_pid(v)\n",
    "        metadata = download_playlist_metadata(user='thesoundsofspotify', \n",
    "                                              pid=playlist_id, \n",
    "                                              pname=genre, \n",
    "                                              client=client)\n",
    "        metadata.to_csv(filepath, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def run_data_pipeline(token):\n",
    "    \"\"\"\n",
    "    - scrape genre page urls from everynoise.com/engenremap.html,\n",
    "        save as a list in ../data/raw/everynoise_genre_urls.pkl\n",
    "        \n",
    "    - scrape genre playlist urls from each genre page on everynoise.com,\n",
    "        save as a dictionary in ../data/raw/thesoundsofspotify_playlist_urls.pkl\n",
    "        \n",
    "    - download playlist metadata for each playlist from Spotify,\n",
    "        save as TSV files in ../data/raw/thesoundsofspotify/[genre].tsv\n",
    "        \n",
    "    - download audio_analysis files for each song in a list of playlists\n",
    "        (not necessarily all playlists because there are 100s of 1000s in the full set)\n",
    "        save as audio_analysis dictionaries in ../data/raw/audio_analysis/[song_uri].pkl\n",
    "    \n",
    "    TODO: include a progress indicator?\n",
    "    \"\"\"\n",
    "    genre_urls = scrape_all_links(\n",
    "        'data/raw/everynoise_genre_urls.pkl',\n",
    "        domain='everynoise.com', \n",
    "        index='engenremap.html', \n",
    "        target_pattern='engenremap-[a-z]*')\n",
    "    \n",
    "    genre_playlists = scrape_links_from_each_page(\n",
    "        'data/raw/thesoundsofspotify_playlist_urls.pkl',\n",
    "        urls=genre_urls,\n",
    "        labeler=(lambda url: url.split('/')[-1].split('-')[-1].split('.')[0]),\n",
    "        target_pattern='open.spotify.com')\n",
    "    \n",
    "    sp = spotipy.Spotify(auth=token)\n",
    "    \n",
    "    download_all_genres_metadata(genre_playlists, sp)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T19:57:23.759275Z",
     "start_time": "2019-06-04T19:50:53.821423Z"
    }
   },
   "outputs": [],
   "source": [
    "token=generate_token(username='djconxn')\n",
    "run_data_pipeline(token)\n",
    "\n",
    "# all genre urls from Every Noise are now saved as a list in 'data/raw/everynoise_genre_urls.pkl'\n",
    "# all genres' playlist links are saved in a dictionary in 'data/raw/thesoundsofspotify_playlist_urls.pkl'\n",
    "# each genre's playlist metadata is saved in 'data/interim/genre_metadata/{genre}_metadata.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T19:11:30.273764Z",
     "start_time": "2019-07-24T19:11:30.226299Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/raw/thesoundsofspotify_playlist_urls.pkl', 'rb') as urls:\n",
    "    genre_playlists = pickle.load(urls)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vinyl",
   "language": "python",
   "name": "vinyl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "249.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
