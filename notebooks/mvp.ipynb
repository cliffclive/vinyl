{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "Vinyl is a sequential deep learning model (recursive neural network or RNN) that classifies the genre (or genres) of a song based on its groove, where the groove is defined by the musical qualities of the notes played in a song most closely related to the rhythm and beat.\n",
    "\n",
    "I collect data from Spotify for playlists from close to 3,000 musical genres, and obtain audio analysis data for 100-200 songs per genre. By training a specialized RNN on a sequence of each note played in each song, I build a model that is able to identify the genres most strongly represented by a song. \n",
    "\n",
    "Genres are not well defined at a fine level of detail, and this methodology is intended to use the vague boundaries between musical genres to produce a map or genealogy of the evolution of musical styles. This is useful for understanding the history and development of music, and for helping listeners to explore new musical styles that should be similar to their tastes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain the Data\n",
    "\n",
    "To build this model, I pull data from a website called [Every Noise At Once](http://everynoise.com/engenremap.html), which maps out close to 3,000 genres of music in a space that is roughly characterized by instrumentation that trends from organic to electric on the vertical axis, anda musical quality that ranges from dense and atmospheric to spiky and bouncy along the horizontal axis.\n",
    "\n",
    "Each of these genres has its own page that contains a word cloud of popular artists in the genre, as well as links to Spotify playlists with 100-200 songs that represent the genre's style. I scrape Every Noise to collect a list of playlist URIs for each genre, and then I use the Spotify API to collect audio analysis files for the songs in each playlist.\n",
    "\n",
    "*After completing this step, be sure to edit `references/data_dictionary` to include descriptions of where you obtained your data and what information it contains.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0,
     7,
     32,
     59,
     93,
     110
    ]
   },
   "outputs": [],
   "source": [
    "## %%writefile ../src/data/make_dataset.py\n",
    "\n",
    "import os, pickle, re, requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def load_or_make(filepath, creator, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Loads data that is pickled at filepath if filepath exists;\n",
    "    otherwise, calls creator(*args, **kwargs) to create the data \n",
    "    and pickle it at filepath.\n",
    "    Returns the data in either case.\n",
    "    \n",
    "    Inputs:\n",
    "    - filepath: path to where data is / should be stored\n",
    "    - creator: function to create data if it is not already pickled\n",
    "    - *args, **kwargs: arguments passed to creator()\n",
    "    \n",
    "    Outputs:\n",
    "    - item: the data that is stored at filepath\n",
    "    \"\"\"\n",
    "    if os.path.isfile(filepath):\n",
    "        with open(filepath, 'rb') as pkl:\n",
    "            item = pickle.load(pkl)\n",
    "    else:\n",
    "        item = creator(*args, **kwargs)\n",
    "        with open(filepath, 'wb') as pkl:\n",
    "            pickle.dump(item, pkl)\n",
    "    return item\n",
    "\n",
    "\n",
    "def scrape_all_links(domain, index, target_pattern):\n",
    "    \"\"\"\n",
    "    Scrapes a website and compiles a list of urls that match a target pattern.\n",
    "    \n",
    "    Inputs: \n",
    "    - domain: domain of the website you want to scrape\n",
    "    - index: path to the page that you want to scrape from `domain`\n",
    "    - target_pattern: regex that specifies the types of links you want to collect\n",
    "    \n",
    "    Outputs:\n",
    "    - target_urls: list of all the links on domain/index that match target_pattern\n",
    "    \"\"\"\n",
    "    main_page = '/'.join(['http:/', domain, index])\n",
    "    response = requests.get(main_page)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise ConnectionError(f\"Failed to connect to {main_page}.\")\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "    target_regex = re.compile(target_pattern)\n",
    "    target_urls = ['/'.join(['http:/', domain, x['href']])\n",
    "                    for x in soup.find_all('a', {'href':target_regex})]\n",
    "\n",
    "    return target_urls\n",
    "\n",
    "\n",
    "def scrape_links_from_each_page(urls, target_pattern, labeler=(lambda x:x)):\n",
    "    \"\"\"\n",
    "    Loops over a list of urls and finds links that matches a target pattern from each page.\n",
    "    \n",
    "    Inputs:\n",
    "    - urls: the list of urls to scrape links from\n",
    "    - target_pattern: regex that specifies the types of links you want to collect\n",
    "    - labeler: function that parses a url and returns a label for that page\n",
    "    \n",
    "    Outputs:\n",
    "    - links: a dictionary with key/value pairs {url_label:[scraped_links]}\n",
    "    \"\"\"\n",
    "    links = {}\n",
    "\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        label = labeler(url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise ConnectionError(f\"Failed to connect to {url}.\")\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "        target_regex = re.compile(target_pattern)\n",
    "        target_urls = [x['href'] for x in soup.find_all('a', {'href':target_regex})]\n",
    "\n",
    "        links[label] = target_urls\n",
    "    \n",
    "    return links\n",
    "\n",
    "## I DON'T KNOW HOW TO AUTHENTICATE A SPOTIFY CLIENT FROM WITHIN A JUPYTER NOTEBOOK!\n",
    "## I'LL HAVE TO DO THE NEXT PARTS FROM THE COMMAND LINE.\n",
    "\n",
    "# NOT YET IMPLEMENTED\n",
    "def download_playlist_metadata(playlists, spotify_client, data_dir):\n",
    "    \"\"\"\n",
    "    Downloads metadata of all songs from each Spotify playlist in playlist_urls,\n",
    "    and saves them in DataFrames in data_dir.\n",
    "    \n",
    "    Inputs:\n",
    "    - playlists: a dictionary with k/v pairs {playlist_name:playlist_url}\n",
    "    - spotify_client: a Spotify API client created and authenticated using spotipy\n",
    "    - data_dir: the directory where metadata DataFrames are to be stored\n",
    "    \n",
    "    Outputs:\n",
    "    - none, but DataFrames are written as TSV files in data_dir\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# NOT YET IMPLEMENTED\n",
    "def download_audio_analysis_files(playlist_df, spotify_client, data_dir):\n",
    "    \"\"\"\n",
    "    Downloads Spotify audio analysis files for all songs in a Spotify playlist\n",
    "    and pickles them in the data_dir directory. Note that the Spotify client\n",
    "    must be created and authenticated in the environment outside of this script.\n",
    "\n",
    "    TODO: find out how to programmatically authenticate a Spotify client object.\n",
    "    \n",
    "    Inputs:\n",
    "    - playlist_df: a DataFrame containing metadata from songs of a Spotify playlist\n",
    "    - spotify_client: a Spotify API client created and authenticated using spotipy\n",
    "    - data_dir: the directory where audio analysis data is to be stored\n",
    "    \n",
    "    Outputs:\n",
    "    - none, but audio analysis files are downloaded and pickled in data_dir\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "        \n",
    "def run():\n",
    "    \"\"\"\n",
    "    - scrape genre page urls from everynoise.com/engenremap.html,\n",
    "        save as a list in ../data/raw/everynoise_genre_urls.pkl\n",
    "        \n",
    "    - scrape genre playlist urls from each genre page on everynoise.com,\n",
    "        save as a dictionary in ../data/raw/thesoundsofspotify_playlist_urls.pkl\n",
    "        \n",
    "    - download playlist metadata for each playlist from Spotify,\n",
    "        save as TSV files in ../data/raw/thesoundsofspotify/[genre].tsv\n",
    "        \n",
    "    - download audio_analysis files for each song in a list of playlists\n",
    "        (not necessarily all playlists because there are 100s of 1000s in the full set)\n",
    "        save as audio_analysis dictionaries in ../data/raw/audio_analysis/[song_uri].pkl\n",
    "    \n",
    "    TODO: include a progress indicator\n",
    "    \"\"\"\n",
    "    genre_urls = load_or_make(\n",
    "        '../data/raw/everynoise_genre_urls.pkl',\n",
    "        scrape_all_links, \n",
    "        domain='everynoise.com', \n",
    "        index='engenremap.html', \n",
    "        target_pattern='engenremap-[a-z]*')\n",
    "    \n",
    "    genre_playlists = load_or_make(\n",
    "        '../data/raw/thesoundsofspotify_playlist_urls.pkl',\n",
    "        scrape_links_from_each_page,\n",
    "        urls=genre_urls,\n",
    "        labeler=(lambda url: url.split('/')[-1].split('-')[-1].split('.')[0]),\n",
    "        target_pattern='open.spotify.com')\n",
    "\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     7,
     28,
     50,
     62
    ]
   },
   "outputs": [],
   "source": [
    "## %%writefile ../src/data/spotify_metadata.py\n",
    "\n",
    "## RUN THIS CODE IN AN IPYTHON SESSION WITH AN AUTHENTICATED SPOTIFY CLIENT\n",
    "## TODO: FIGURE OUT HOW TO AUTHENTICATE FROM INSIDE A RUNNING PYTHON PROGRAM\n",
    "## WITHOUT HAVING TO COPY/PASTE URI FROM BROWSER INTO PROMPT\n",
    "\n",
    "\n",
    "def parse_track_tags(track):\n",
    "    '''\n",
    "    Parse metadata for a spotify track, return as a dict\n",
    "    From a user_playlist json file, a track can be found via:\n",
    "        user_playlist['tracks']['items'][i]\n",
    "    '''\n",
    "    return {\n",
    "        'id': track['id'],\n",
    "        'album': track['album']['name'],\n",
    "        'track': track['track_number'],\n",
    "        'title': track['name'],\n",
    "        'artist': track['artists'][0]['name'],\n",
    "        'duration': int(track['duration_ms']/1000),\n",
    "        'preview_mp3': track['preview_url'],\n",
    "        'is_explicit': track['explicit'],\n",
    "        'isrc_number': track['external_ids']['isrc'],\n",
    "        'release_date': track['album']['release_date'],\n",
    "        'cover_art_url': track['album']['images'][0]['url']\n",
    "    }\n",
    "\n",
    "\n",
    "def get_playlist_metadata(tracks, client):\n",
    "    \"\"\"\n",
    "    Downloads metadata for each track in tracks, using Spotify API client.\n",
    "    Downloads audio features for each track and appends to metadata df.\n",
    "    \n",
    "    Inputs:\n",
    "    - tracks: Spotify tracks node from playlist json data\n",
    "    - client: Spotify API client\n",
    "    \n",
    "    Outputs:\n",
    "    - metadata_df: a DataFrame merged from playlist metadata and \n",
    "        audio features downloaded for each song in the playlist\n",
    "    \"\"\"\n",
    "    metadata = [parse_track_tags(item['track']) for item in tracks['items']]\n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    song_ids = list(metadata_df['id'])\n",
    "    features_df = pd.DataFrame(client.audio_features(song_ids))\n",
    "    metadata_df = pd.merge(metadata_df, features_df)\n",
    "\n",
    "    return metadata_df\n",
    "\n",
    "\n",
    "def download_tracks_analysis(metadata, client):\n",
    "    \"\"\"\n",
    "    Download audio analysis files for all tracks in playlist, using Spotify API client.\n",
    "    \"\"\"\n",
    "    for index, row in metadata.iterrows():\n",
    "        filepath = f'data/raw/audio_analysis/{row.id}.pkl'\n",
    "        if not os.path.isfile(filepath):\n",
    "            analysis = client.audio_analysis(row.id)\n",
    "            with open(filepath, 'wb') as pkl:\n",
    "                pickle.dump(analysis, pkl)\n",
    "\n",
    "\n",
    "def get_sos_playlist_id(genre_playlists, genre):\n",
    "    \"\"\"\n",
    "    Parses out the Spotify uri of a genre's playlist created by The Sounds Of Spotify,\n",
    "    from a list of urls on that genre's subpage on Every Noise At Once.\n",
    "    \n",
    "    Inputs:\n",
    "    - genre_playlists: a dict of format {genre:[playlist_urls]} for every genre on EveryNoise\n",
    "    - genre: a string, genre name as it appears on EveryNoise (lowercase, hypen-joined)\n",
    "    \n",
    "    Outputs:\n",
    "    - playlist_id: a Spotify uri to that playlist\n",
    "    \"\"\"\n",
    "    sos_playlist = [x for x in genre_playlists[genre] if 'thesoundsofspotify' in x][0]\n",
    "    return sos_playlist.split('/')[-1]\n",
    "\n",
    "\n",
    "def run_playlist_metadata_pipeline(genre, genre_playlists, spotipy_client):\n",
    "    \"\"\"\n",
    "    This is a pipeline to\n",
    "        get the playlist id for a genre name,\n",
    "        download playlist tracks,\n",
    "        download and save playlist metadata,\n",
    "        download and save tracks audio analysis        \n",
    "    \"\"\"\n",
    "    playlist_id = get_sos_playlist_id(genre_playlists, genre)\n",
    "    tracks = spotipy_client.user_playlist('thesoundsofspotify', playlist_id, fields=\"tracks,next\")['tracks']\n",
    "    with open(f'data/raw/playlist_json/{genre}.pkl', 'wb') as pkl:\n",
    "        pickle.dump(tracks, pkl)\n",
    "    metadata = get_playlist_metadata(tracks, spotipy_client)\n",
    "    metadata.to_csv(f'data/interim/{genre}_metadata.tsv', sep='\\t')\n",
    "    download_tracks_analysis(metadata, spotipy_client)\n",
    "\n",
    "#run_playlist_metadata_pipeline('techno', genre_playlists, spotipy_client)\n",
    "#run_playlist_metadata_pipeline('opera', genre_playlists, spotipy_client)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'download_user_playlist_tracks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5f49b7c73b9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m genre_tracks = load_or_make(\n\u001b[1;32m      3\u001b[0m     \u001b[0;34mf'data/raw/playlist_json/{genre}.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdownload_user_playlist_tracks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspotipy_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'thesoundsofspotify'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'download_user_playlist_tracks' is not defined"
     ]
    }
   ],
   "source": [
    "genre = \"techno\"\n",
    "genre_tracks = load_or_make(\n",
    "    f'data/raw/playlist_json/{genre}.pkl',\n",
    "    download_user_playlist_tracks,\n",
    "    client = spotipy_client,\n",
    "    user = 'thesoundsofspotify', \n",
    "    playlist_id = pl_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "```\n",
    "data/\n",
    "|-raw/everynoise_genre_urls.pkl\n",
    "| |  /thesoundsofspotify_playlist_urls.pkl\n",
    "| |--/audio_analysis/{uri}.pkl\n",
    "| `--/playlist_json/{genre}.pkl\n",
    "`-interim/{genre}_metadata.tsv\n",
    "\n",
    "TODO: Write a function that walks the data directory tree and prints this output.\n",
    "```\n",
    "\n",
    "Descriptions:\n",
    "- **everynoise_genre_urls.pkl**: `list` of urls for all genre subpages on everynoise.com\n",
    "- **thesoundsofspotify_playlist_urls.pkl**: `dict` of genre:urls for all EveryNoise genres; urls point to Spotify playlists by users \"The Sounds of Spotify\" and \"Particle Detector\". TSOS playlists are curated lists of songs that exemplify a genre. PD playlists track most-listened to songs of fans of that genre. TSOS is more useful for genre classification because PD playlists are likely to pick up songs in related genres, not the ones we are predicting. (Still useful, but for a different problem.)\n",
    "- **{uri}.pkl**: `dict` of Audio Analysis JSON file downloaded from Spotify for song {uri}\n",
    "- **{genre}.pkl**: `dict` of a playlist JSON data file downloaded from Spotify\n",
    "- **{genre}\\_metadata.tsv**: `DataFrame` compiled from track metadata parsed from playlist JSON, merged with a DataFrame of Audio Features data downloaded from Spotify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrub the Data\n",
    "\n",
    "So far I have:\n",
    "- a directory full of `genre_metadata` DataFrames, the id of which maps to the filenames in...\n",
    "- a directory full of pickled audio_analysis files for songs in each of the genre playlists.\n",
    "\n",
    "I want to create a DataFrame of cleaned audio analysis data for each song. Eventually it may be interesting to add the Spotify Audio Features as an additional vector of features in the final layer of my neural network. But for now I will stick to the audio analysis data.\n",
    "\n",
    "*The end goal of this step is to produce a [design matrix](https://en.wikipedia.org/wiki/Design_matrix), containing one column for every variable that you are modeling, including a column for the outputs, and one row for every observation in your data set. It needs to be in a format that won't cause any problems as you visualize and model your data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def parse_groove_from_audio_analysis(analysis):\n",
    "    bars = pd.DataFrame(analysis['bars'])\n",
    "    segments = pd.DataFrame(analysis['segments'])\n",
    "\n",
    "    bars['bar'] = bars.index + 1\n",
    "    segments.head()\n",
    "    segments['amplitude'] = segments.loudness_max - segments.loudness_start\n",
    "    segments['transient'] = segments.start + segments.loudness_max_time\n",
    "    segments['attack'] = segments.loudness_max_time\n",
    "\n",
    "    notes = segments[['transient', 'amplitude', 'duration', 'attack']]\n",
    "\n",
    "    groove = pd.merge_asof(notes, bars, left_on='transient', right_on='start')\n",
    "    groove.rename(index=str, columns={\"duration_x\": \"duration\"}, inplace=True)\n",
    "    groove = groove.dropna().drop('confidence', axis=1).reset_index(drop=True)\n",
    "    groove['beat'] = (groove.transient - groove.start) / groove.duration_y\n",
    "    groove.beat = round(groove.beat, 2)\n",
    "    groove = groove[groove.beat <= 1]\n",
    "\n",
    "    all_beats = pd.Series(np.linspace(0,0.99,100))\n",
    "    missing_beats = all_beats[~all_beats.isin(groove.beat)]\n",
    "\n",
    "    fill_groove = pd.DataFrame(np.zeros([missing_beats.shape[0], groove.shape[1]]),\n",
    "                               columns=groove.columns,\n",
    "                               index=missing_beats.index)\n",
    "\n",
    "    # shouldn't have to round this, but sometimes floating point errors creep in\n",
    "    fill_groove[\"beat\"] = round(missing_beats, 2)\n",
    "    \n",
    "    #groove = pd.concat([groove, fill_groove])\n",
    "    groove.reset_index()\n",
    "    groove = groove.pivot_table(index='start', columns='beat', \n",
    "                                values=['amplitude', 'attack', 'duration'])\n",
    "\n",
    "    groove.fillna(0, inplace=True)\n",
    "    \n",
    "    groove = pd.DataFrame(groove.to_records())\n",
    "    groove.columns = [col.replace(\"('\", \"\").replace(\"', \", \"_\").replace(\")\", \"\") for col in groove.columns]\n",
    "    \n",
    "    return groove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# data processing pipeline\n",
    "# get names of all genres with a metadata tsv file in data/interim\n",
    "# \n",
    "\n",
    "def get_genres_with_metadata(metadata_dir='../data/interim'):\n",
    "\n",
    "    metadata_paths = [x for x in os.listdir(metadata_dir) if x.endswith('_metadata.tsv')]\n",
    "    genre_ids = {x.split('_metadata')[0] : \n",
    "                 pd.read_csv(os.path.join(metadata_dir, x), sep='\\t').id\n",
    "                 for x in metadata_paths}\n",
    "    return genre_ids\n",
    "\n",
    "\n",
    "def process_grooves_from_audio_analysis(audio_analysis_dir='../data/raw/audio_analysis', \n",
    "                                        features_dir='../data/processed'):\n",
    "    for genre in genre_ids.keys():\n",
    "        for i in genre_ids[genre]:\n",
    "            audio_analysis_path = os.path.join(audio_analysis_dir, i + '.pkl')\n",
    "            groove_filepath = '_'.join([genre, i]) + '.tsv'\n",
    "            if os.path.isfile(groove_filepath):\n",
    "                continue\n",
    "\n",
    "            with open(audio_analysis_path, 'rb') as pkl:\n",
    "                audio_analysis = pickle.load(pkl)\n",
    "            groove = parse_groove_from_audio_analysis(audio_analysis)\n",
    "            groove.to_csv(os.path.join(features_dir, groove_filepath), sep='\\t')\n",
    "\n",
    "#os.listdir('../data/raw/audio_analysis')\n",
    "\n",
    "def concatenate_supergroove_dataframe(features_dir):\n",
    "    grooves = []\n",
    "    for groove_tsv in os.listdir(features_dir):\n",
    "        if not groove_tsv.endswith('.tsv'):\n",
    "            print(f\"{groove_tsv} is not a tsv file\")\n",
    "            continue\n",
    "        groove_path = os.path.join(features_dir, groove_tsv)\n",
    "        groove = pd.read_csv(groove_path, sep=\"\\t\", index_col=0)\n",
    "        #if not set(list(groove.columns)) == set(cols):\n",
    "        #    print(f'{groove_tsv} is not a proper groove DataFrame')\n",
    "        #    continue\n",
    "        genre, song_id = groove_tsv.split('_')\n",
    "        song_id = song_id.split('.')[0]\n",
    "        groove['genre'] = genre\n",
    "        groove['song_id'] = song_id\n",
    "        grooves.append(groove)\n",
    "    return pd.concat(grooves)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"../data/processed/operatechno.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile ../src/features/build_features.py\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# helper functions go here\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that read files from data/raw, cleans them,\n",
    "    and converts the data into a design matrix that is ready for modeling.\n",
    "    \"\"\"\n",
    "    # clean_dataset_1('data/raw', filename)\n",
    "    # clean_dataset_2('data/raw', filename)\n",
    "    # save_cleaned_data_1('data/interim', filename)\n",
    "    # save_cleaned_data_2('data/interim', filename)\n",
    "    # build_features()\n",
    "    # save_features('data/processed')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We're not quite there yet\n",
    "\n",
    "Need to decide how to encode the timing data, and whether to include sections. (Do bar counts restart with new sections?)\n",
    "\n",
    "Also I should detect the key of the song and cycle the pitches so that pitch_00 is tonic.\n",
    "\n",
    "Base case SVM model: \n",
    "- loudness_delta = loundess_max - loudness_start\n",
    "- fill in missing tatums with zeroes for loudness_delta, pitches, timbres, duration, attack(?)\n",
    "- missing tatum time = (previous tatum time) + ((previous matching tatum count) - 1) duration\n",
    "- observation = delta, attack, duration, pitches, timbres for all of the tatums in one bar\n",
    "- target = genre of that bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Before moving on to exploratory analysis, write down some notes about challenges encountered while working with this data that might be helpful for anyone else (including yourself) who may work through this later on.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Data\n",
    "\n",
    "*Before you start exploring the data, write out your thought process about what you're looking for and what you expect to find. Take a minute to confirm that your plan actually makes sense.*\n",
    "\n",
    "*Calculate summary statistics and plot some charts to give you an idea what types of useful relationships might be in your dataset. Use these insights to go back and download additional data or engineer new features if necessary. Not now though... remember we're still just trying to finish the MVP!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-6a0585a3abd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, normed, data, **kwargs)\u001b[0m\n\u001b[1;32m   2657\u001b[0m         \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m         \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(self, x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, normed, **kwargs)\u001b[0m\n\u001b[1;32m   6665\u001b[0m                 patch = _barfunc(bins[:-1]+boffset, height, width,\n\u001b[1;32m   6666\u001b[0m                                  \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'center'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6667\u001b[0;31m                                  color=c, **{bottom_kwarg: bottom})\n\u001b[0m\u001b[1;32m   6668\u001b[0m                 \u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6669\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstacked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mbar\u001b[0;34m(self, x, height, width, bottom, align, **kwargs)\u001b[0m\n\u001b[1;32m   2339\u001b[0m             \u001b[0mymin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mymin\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataLim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintervaly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mymin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2341\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoscale_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2343\u001b[0m         \u001b[0mbar_container\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBarContainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrorbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mautoscale_view\u001b[0;34m(self, tight, scalex, scaley)\u001b[0m\n\u001b[1;32m   2426\u001b[0m             \u001b[0mstickies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msticky_edges\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0martist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2427\u001b[0m             \u001b[0mx_stickies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msticky\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msticky\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstickies\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2428\u001b[0;31m             \u001b[0my_stickies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msticky\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msticky\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstickies\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_xscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'log'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2430\u001b[0m                 \u001b[0mx_stickies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxs\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_stickies\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data.genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile ../src/visualization/visualize.py\n",
    "\n",
    "# imports\n",
    "# helper functions go here\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that read files from data/processed,\n",
    "    calculates descriptive statistics for the population, and plots charts\n",
    "    that visualize interesting relationships between features.\n",
    "    \"\"\"\n",
    "    # data = load_features('data/processed')\n",
    "    # describe_features(data, 'reports/')\n",
    "    # generate_charts(data, 'reports/figures/')\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What did you learn? What relationships do you think will be most helpful as you build your model?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model the Data\n",
    "\n",
    "*Describe the algorithm or algorithms that you plan to use to train with your data. How do these algorithms work? Why are they good choices for this data and problem space?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def songid_partition(groove_df, test_size=0.2):\n",
    "    ids = groove_df.song_id.unique()\n",
    "    nids = ids.shape[0]\n",
    "    a = np.zeros(nids, dtype=int)\n",
    "    ntest = int(nids * test_size)\n",
    "    a[:ntest] = 1\n",
    "    np.random.shuffle(a)\n",
    "    a = a.astype(bool)\n",
    "    train_ids = ~a\n",
    "    return ids[train_ids], ids[a]\n",
    "\n",
    "def groove_train_test_split(groove_df, target_genre, test_size=0.2):\n",
    "    train_ids, test_ids = songid_partition(groove_df, test_size)\n",
    "    train_set = groove_df[groove_df.song_id.isin(train_ids)]\n",
    "    X_train = train_set.drop(['genre', 'song_id', 'start'], axis=1)\n",
    "    y_train = np.zeros_like(train_set['genre']).astype('int')\n",
    "    y_train[train_set.genre == target_genre] = 1\n",
    "    test_set = groove_df[groove_df.song_id.isin(test_ids)]\n",
    "    X_test = test_set.drop(['genre', 'song_id', 'start'], axis=1)\n",
    "    y_test = np.zeros_like(test_set['genre']).astype('int')\n",
    "    y_test[test_set.genre == target_genre] = 1\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = groove_train_test_split(data.dropna(), 'techno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50)\n",
    "pca.fit(X_train)\n",
    "pcafeatures_train = pca.transform(X_train)\n",
    "pcafeatures_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cliffclive/anaconda3/envs/metis/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(pcafeatures_train, y_train)\n",
    "f1_score(lr.predict(pcafeatures_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12157,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "          431]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile ../src/models/train_model.py\n",
    "\n",
    "# imports\n",
    "# helper functions go here\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that read files from data/processed,\n",
    "    calculates descriptive statistics for the population, and plots charts\n",
    "    that visualize interesting relationships between features.\n",
    "    \"\"\"\n",
    "    # data = load_features('data/processed/')\n",
    "    # train, test = train_test_split(data)\n",
    "    # save_train_test(train, test, 'data/processed/')\n",
    "    # model = build_model()\n",
    "    # model.fit(train)\n",
    "    # save_model(model, 'models/')\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile ../src/models/predict_model.py\n",
    "\n",
    "# imports\n",
    "# helper functions go here\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that read files from data/processed,\n",
    "    calculates descriptive statistics for the population, and plots charts\n",
    "    that visualize interesting relationships between features.\n",
    "    \"\"\"\n",
    "    # test_X, test_y = load_test_data('data/processed')\n",
    "    # trained_model = load_model('models/')\n",
    "    # predictions = trained_model.predict(test_X)\n",
    "    # metrics = evaluate(test_y, predictions)\n",
    "    # save_metrics('reports/')\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write down any thoughts you may have about working with these algorithms on this data. What other ideas do you want to try out as you iterate on this pipeline?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret the Model\n",
    "\n",
    "_Write up the things you learned, and how well your model performed. Be sure address the model's strengths and weaknesses. What types of data does it handle well? What types of observations tend to give it a hard time? What future work would you or someone reading this might want to do, building on the lessons learned and tools developed in this project?_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
