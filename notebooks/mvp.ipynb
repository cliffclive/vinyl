{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "Vinyl is a sequential deep learning model (recursive neural network or RNN) that characterizes the genre (or genres) of a song based on its groove, where the groove is defined by the timing, tempo, and musical qualities of the notes played in a song.\n",
    "\n",
    "I collect data from Spotify for playlists from close to 3,000 musical genres, and obtain audio analysis data for 100-200 songs per genre. By training an RNN a sequence of each note played in each song, I build a model that is able to identify the genres that influence a song. \n",
    "\n",
    "Genres are not well defined at a fine level of detail, and this methodology is intended to use the vague boundaries between musical genres to produce a map or genealogy of the evolution of musical styles. This is useful for understanding the history and development of music, and for helping listeners to explore new musical styles that should be similar to their tastes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain the Data\n",
    "\n",
    "To build this model, I pull data from a website called [Every Noise At Once](http://everynoise.com/engenremap.html), which maps out close to 3,000 genres of music in a space that roughly characterized by instrumentation that trends from organic to electric on the vertical axis, anda musical quality that ranges from dense and atmospheric to spiky and bouncy along the horizontal axis.\n",
    "\n",
    "Each of these genres has its own page that contains a word cloud of popular artists in the genre, as well as links to Spotify playlists with 100-200 songs that represent the genre's style. I scrape Every Noise to collect a list of playlist URIs for each genre, and then I use the Spotify API to collect audio analysis files for the songs in each playlist.\n",
    "\n",
    "*After completing this step, be sure to edit `references/data_dictionary` to include descriptions of where you obtained your data and what information it contains.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     7,
     32,
     59,
     93,
     109
    ]
   },
   "outputs": [],
   "source": [
    "## %%writefile ../src/data/make_dataset.py\n",
    "\n",
    "import os, pickle, re, requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def load_or_make(filepath, creator, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Loads data that is pickled at filepath if filepath exists;\n",
    "    otherwise, calls creator(*args, **kwargs) to create the data \n",
    "    and pickle it at filepath.\n",
    "    Returns the data in either case.\n",
    "    \n",
    "    Inputs:\n",
    "    - filepath: path to where data is / should be stored\n",
    "    - creator: function to create data if it is not already pickled\n",
    "    - *args, **kwargs: arguments passed to creator()\n",
    "    \n",
    "    Outputs:\n",
    "    - item: the data that is stored at filepath\n",
    "    \"\"\"\n",
    "    if os.path.isfile(filepath):\n",
    "        with open(filepath, 'rb') as pkl:\n",
    "            item = pickle.load(pkl)\n",
    "    else:\n",
    "        item = creator(*args, **kwargs)\n",
    "        with open(filepath, 'wb') as pkl:\n",
    "            pickle.dump(item, pkl)\n",
    "    return item\n",
    "\n",
    "\n",
    "def scrape_all_links(domain, index, target_pattern):\n",
    "    \"\"\"\n",
    "    Scrapes a website and compiles a list of urls that match a target pattern.\n",
    "    \n",
    "    Inputs: \n",
    "    - domain: domain of the website you want to scrape\n",
    "    - index: path to the page that you want to scrape from `domain`\n",
    "    - target_pattern: regex that specifies the types of links you want to collect\n",
    "    \n",
    "    Outputs:\n",
    "    - target_urls: list of all the links on domain/index that match target_pattern\n",
    "    \"\"\"\n",
    "    main_page = '/'.join(['http:/', domain, index])\n",
    "    response = requests.get(main_page)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise ConnectionError(f\"Failed to connect to {main_page}.\")\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "    target_regex = re.compile(target_pattern)\n",
    "    target_urls = ['/'.join(['http:/', domain, x['href']])\n",
    "                    for x in soup.find_all('a', {'href':target_regex})]\n",
    "\n",
    "    return target_urls\n",
    "\n",
    "\n",
    "def scrape_links_from_each_page(urls, target_pattern, labeler=(lambda x:x)):\n",
    "    \"\"\"\n",
    "    Loops over a list of urls and finds links that matches a target pattern from each page.\n",
    "    \n",
    "    Inputs:\n",
    "    - urls: the list of urls to scrape links from\n",
    "    - target_pattern: regex that specifies the types of links you want to collect\n",
    "    - labeler: function that parses a url and returns a label for that page\n",
    "    \n",
    "    Outputs:\n",
    "    - links: a dictionary with key/value pairs {url_label:[scraped_links]}\n",
    "    \"\"\"\n",
    "    links = {}\n",
    "\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        label = labeler(url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise ConnectionError(f\"Failed to connect to {url}.\")\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "        target_regex = re.compile(target_pattern)\n",
    "        target_urls = [x['href'] for x in soup.find_all('a', {'href':target_regex})]\n",
    "\n",
    "        links[label] = target_urls\n",
    "    \n",
    "    return links\n",
    "\n",
    "\n",
    "## I DON'T KNOW HOW TO AUTHENTICATE A SPOTIFY CLIENT FROM WITHIN A JUPYTER NOTEBOOK!\n",
    "## I'LL HAVE TO DO THE NEXT PARTS FROM THE COMMAND LINE.\n",
    "\n",
    "def download_playlist_metadata(playlists, spotify_client, data_dir):\n",
    "    \"\"\"\n",
    "    Downloads metadata of all songs from each Spotify playlist in playlist_urls,\n",
    "    and saves them in DataFrames in data_dir.\n",
    "    \n",
    "    Inputs:\n",
    "    - playlists: a dictionary with k/v pairs {playlist_name:playlist_url}\n",
    "    - spotify_client: a Spotify API client created and authenticated using spotipy\n",
    "    - data_dir: the directory where metadata DataFrames are to be stored\n",
    "    \n",
    "    Outputs:\n",
    "    - none, but DataFrames are written as TSV files in data_dir\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def download_audio_analysis_files(playlist_df, spotify_client, data_dir):\n",
    "    \"\"\"\n",
    "    Downloads Spotify audio analysis files for all songs in a Spotify playlist\n",
    "    and pickles them in the data_dir directory. Note that the Spotify client\n",
    "    must be created and authenticated in the environment outside of this script.\n",
    "\n",
    "    TODO: find out how to programmatically authenticate a Spotify client object.\n",
    "    \n",
    "    Inputs:\n",
    "    - playlist_df: a DataFrame containing metadata from songs of a Spotify playlist\n",
    "    - spotify_client: a Spotify API client created and authenticated using spotipy\n",
    "    - data_dir: the directory where audio analysis data is to be stored\n",
    "    \n",
    "    Outputs:\n",
    "    - none, but audio analysis files are downloaded and pickled in data_dir\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "        \n",
    "def run():\n",
    "    \"\"\"\n",
    "    - scrape genre page urls from everynoise.com/engenremap.html,\n",
    "        save as a list in ../data/raw/everynoise_genre_urls.pkl\n",
    "        \n",
    "    - scrape genre playlist urls from each genre page on everynoise.com,\n",
    "        save as a dictionary in ../data/raw/thesoundsofspotify_playlist_urls.pkl\n",
    "        \n",
    "    - download playlist metadata for each playlist from Spotify,\n",
    "        save as TSV files in ../data/raw/thesoundsofspotify/[genre].tsv\n",
    "        \n",
    "    - download audio_analysis files for each song in a list of playlists\n",
    "        (not necessarily all playlists because there are 100s of 1000s in the full set)\n",
    "        save as audio_analysis dictionaries in ../data/raw/audio_analysis/[song_uri].pkl\n",
    "    \n",
    "    TODO: include a progress indicator\n",
    "    \"\"\"\n",
    "    genre_urls = load_or_make(\n",
    "        '../data/raw/everynoise_genre_urls.pkl',\n",
    "        scrape_all_links, \n",
    "        domain='everynoise.com', \n",
    "        index='engenremap.html', \n",
    "        target_pattern='engenremap-[a-z]*')\n",
    "    \n",
    "    genre_playlists = load_or_make(\n",
    "        '../data/raw/thesoundsofspotify_playlist_urls.pkl',\n",
    "        scrape_links_from_each_page,\n",
    "        urls=genre_urls,\n",
    "        labeler=(lambda url: url.split('/')[-1].split('-')[-1].split('.')[0]),\n",
    "        target_pattern='open.spotify.com')\n",
    "\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_urls = load_or_make(\n",
    "    '../data/raw/everynoise_genre_urls.pkl',\n",
    "    scrape_all_links, \n",
    "    domain='everynoise.com', \n",
    "    index='engenremap.html', \n",
    "    target_pattern='engenremap-[a-z]*')\n",
    "\n",
    "genre_playlists = load_or_make(\n",
    "    '../data/raw/thesoundsofspotify_playlist_urls.pkl',\n",
    "    scrape_links_from_each_page,\n",
    "    urls=genre_urls,\n",
    "    labeler=(lambda url: url.split('/')[-1].split('-')[-1].split('.')[0]),\n",
    "    target_pattern='open.spotify.com')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     25
    ]
   },
   "outputs": [],
   "source": [
    "## RUN THIS CODE IN AN IPYTHON SESSION WITH AN AUTHENTICATED SPOTIFY CLIENT\n",
    "## TODO: FIGURE OUT HOW TO AUTHENTICATE FROM INSIDE A RUNNING PYTHON PROGRAM\n",
    "## WITHOUT HAVING TO COPY/PASTE URI FROM BROWSER INTO PROMPT\n",
    "\n",
    "def parse_track_tags(track):\n",
    "    '''\n",
    "    Parse metadata for a spotify track, return as a dict\n",
    "    From a user_playlist json file, a track can be found via:\n",
    "        user_playlist['tracks']['items'][i]\n",
    "    '''\n",
    "    return {\n",
    "        'id': track['id'],\n",
    "        'album': track['album']['name'],\n",
    "        'track': track['track_number'],\n",
    "        'title': track['name'],\n",
    "        'artist': track['artists'][0]['name'],\n",
    "        'duration': int(track['duration_ms']/1000),\n",
    "        'preview_mp3': track['preview_url'],\n",
    "        'is_explicit': track['explicit'],\n",
    "        'isrc_number': track['external_ids']['isrc'],\n",
    "        'release_date': track['album']['release_date'],\n",
    "        'cover_art_url': track['album']['images'][0]['url']\n",
    "    }\n",
    "\n",
    "\n",
    "def get_playlist_metadata(tracks, client):\n",
    "    \"\"\"\n",
    "    Downloads metadata for each track in tracks, using Spotify API client.\n",
    "    Downloads audio features for each track and appends to metadata df.\n",
    "    \n",
    "    Inputs:\n",
    "    - tracks: Spotify tracks node from playlist json data\n",
    "    - client: Spotify API client\n",
    "    \n",
    "    Outputs:\n",
    "    - metadata_df: a DataFrame merged from playlist metadata and \n",
    "        audio features downloaded for each song in the playlist\n",
    "    \"\"\"\n",
    "    metadata = [parse_track_tags(item['track']) for item in tracks['items']]\n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    song_ids = list(metadata_df['id'])\n",
    "    features_df = pd.DataFrame(client.audio_features(song_ids))\n",
    "    metadata_df = pd.merge(metadata_df, features_df)\n",
    "\n",
    "    return metadata_df\n",
    "\n",
    "\n",
    "def download_tracks_analysis(metadata, client):\n",
    "    \"\"\"\n",
    "    Download audio analysis files for all tracks in playlist, using Spotify API client.\n",
    "    \"\"\"\n",
    "    for index, row in metadata.iterrows():\n",
    "        filepath = f'data/raw/audio_analysis/{row.id}.pkl'\n",
    "        if not os.path.isfile(filepath):\n",
    "            analysis = client.audio_analysis(row.id)\n",
    "            with open(filepath, 'wb') as pkl:\n",
    "                pickle.dump(analysis, pkl)\n",
    "\n",
    "def download_user_playlist_tracks(client, user, playlist_id):\n",
    "    \"\"\"\n",
    "    Calls Spotify client.user_playlist to get a playlist's JSON metadata, \n",
    "    and only returns the tracks metadata.\n",
    "    \n",
    "    Inputs:\n",
    "    - client: a spotipy Spotify client object\n",
    "    - user: Spotify username of the playlist creator\n",
    "    - playlist_id: the Spotify uri of the playlist\n",
    "    \n",
    "    Outputs:\n",
    "    - tracks\n",
    "    \"\"\"\n",
    "    playlist_json = client.user_playlist(user, playlist_id, fields=\"tracks,next\")\n",
    "    return playlist_json['tracks']\n",
    "\n",
    "\n",
    "def get_sos_playlist_id(genre_playlists, genre):\n",
    "    \"\"\"\n",
    "    Parses out the Spotify uri of a genre's playlist created by The Sounds Of Spotify,\n",
    "    from a list of urls on that genre's subpage on Every Noise At Once.\n",
    "    \n",
    "    Inputs:\n",
    "    - genre_playlists: a dict of format {genre:[playlist_urls]} for every genre on EveryNoise\n",
    "    - genre: a string, genre name as it appears on EveryNoise (lowercase, hypen-joined)\n",
    "    \n",
    "    Outputs:\n",
    "    - playlist_id: a Spotify uri to that playlist\n",
    "    \"\"\"\n",
    "    sos_playlist = [x for x in genre_playlists[genre] if 'thesoundsofspotify' in x][0]\n",
    "    return sos_playlist.split('/')[-1]\n",
    "\n",
    "\n",
    "def run_playlist_metadata_pipeline(genre, genre_playlists, client):\n",
    "    \"\"\"\n",
    "    This is a pipeline to\n",
    "        get the playlist id for a genre name,\n",
    "        download playlist tracks,\n",
    "        download and save playlist metadata,\n",
    "        download and save tracks audio analysis        \n",
    "    \"\"\"\n",
    "    pl_id = get_sos_playlist_id(genre_playlists, genre)\n",
    "    tracks = client.user_playlist('thesoundsofspotify', pl_id, fields=\"tracks,next\")['tracks']\n",
    "    with open(f'data/raw/playlist_json/{genre}.pkl', 'wb') as pkl:\n",
    "        pickle.dump(tracks, pkl)\n",
    "    metadata = get_playlist_metadata(tracks, sp)\n",
    "    metadata.to_csv(f'data/interim/{genre}_metadata.tsv', sep='\\t')\n",
    "    download_tracks_analysis(metadata, sp)\n",
    "\n",
    "run_playlist_metadata_pipeline('techno', genre_playlists, sp)\n",
    "run_playlist_metadata_pipeline('opera', genre_playlists, sp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_tracks = load_or_make(\n",
    "    f'data/raw/playlist_json/{genre}.pkl',\n",
    "    download_user_playlist_tracks,\n",
    "    client = sp,\n",
    "    user = 'thesoundsofspotify', \n",
    "    playlist_id = pl_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "```\n",
    "data/\n",
    "|-raw/everynoise_genre_urls.pkl\n",
    "| |  /thesoundsofspotify_playlist_urls.pkl\n",
    "| |--/audio_analysis/{uri}.pkl\n",
    "| `--/playlist_json/{genre}.pkl\n",
    "`-interim/{genre}_metadata.tsv\n",
    "\n",
    "TODO: Write a function that walks the data directory tree and prints this output.\n",
    "```\n",
    "\n",
    "Descriptions:\n",
    "- **everynoise_genre_urls.pkl**: `list` of urls for all genre subpages on everynoise.com\n",
    "- **thesoundsofspotify_playlist_urls.pkl**: `dict` of genre:urls for all EveryNoise genres; urls point to Spotify playlists by users \"The Sounds of Spotify\" and \"Particle Detector\"\n",
    "- **{uri}.pkl**: `dict` of Audio Analysis JSON file downloaded from Spotify for song {uri}\n",
    "- **{genre}.pkl**: `dict` of a playlist JSON data file downloaded from Spotify\n",
    "- **{genre}\\_metadata.tsv**: `DataFrame` compiled from track metadata parsed from playlist JSON, merged with a DataFrame of Audio Features data downloaded from Spotify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrub the Data\n",
    "\n",
    "*Look through the raw data files and see what you will need to do to them in order to have a workable data set. If your source data is already well-formatted, you may want to ask yourself why it hasn't already been analyzed and what other people may have overlooked when they were working on it. Are there other data sources that might give you more insights on some of the data you have here?*\n",
    "\n",
    "*The end goal of this step is to produce a [design matrix](https://en.wikipedia.org/wiki/Design_matrix), containing one column for every variable that you are modeling, including a column for the outputs, and one row for every observation in your data set. It needs to be in a format that won't cause any problems as you visualize and model your data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile ../src/features/build_features.py\n",
    "\n",
    "# imports\n",
    "# helper functions go here\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that read files from data/raw, cleans them,\n",
    "    and converts the data into a design matrix that is ready for modeling.\n",
    "    \"\"\"\n",
    "    # clean_dataset_1('data/raw', filename)\n",
    "    # clean_dataset_2('data/raw', filename)\n",
    "    # save_cleaned_data_1('data/interim', filename)\n",
    "    # save_cleaned_data_2('data/interim', filename)\n",
    "    # build_features()\n",
    "    # save_features('data/processed')\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Before moving on to exploratory analysis, write down some notes about challenges encountered while working with this data that might be helpful for anyone else (including yourself) who may work through this later on.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Data\n",
    "\n",
    "*Before you start exploring the data, write out your thought process about what you're looking for and what you expect to find. Take a minute to confirm that your plan actually makes sense.*\n",
    "\n",
    "*Calculate summary statistics and plot some charts to give you an idea what types of useful relationships might be in your dataset. Use these insights to go back and download additional data or engineer new features if necessary. Not now though... remember we're still just trying to finish the MVP!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile ../src/visualization/visualize.py\n",
    "\n",
    "# imports\n",
    "# helper functions go here\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that read files from data/processed,\n",
    "    calculates descriptive statistics for the population, and plots charts\n",
    "    that visualize interesting relationships between features.\n",
    "    \"\"\"\n",
    "    # data = load_features('data/processed')\n",
    "    # describe_features(data, 'reports/')\n",
    "    # generate_charts(data, 'reports/figures/')\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What did you learn? What relationships do you think will be most helpful as you build your model?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model the Data\n",
    "\n",
    "*Describe the algorithm or algorithms that you plan to use to train with your data. How do these algorithms work? Why are they good choices for this data and problem space?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile ../src/models/train_model.py\n",
    "\n",
    "# imports\n",
    "# helper functions go here\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that read files from data/processed,\n",
    "    calculates descriptive statistics for the population, and plots charts\n",
    "    that visualize interesting relationships between features.\n",
    "    \"\"\"\n",
    "    # data = load_features('data/processed/')\n",
    "    # train, test = train_test_split(data)\n",
    "    # save_train_test(train, test, 'data/processed/')\n",
    "    # model = build_model()\n",
    "    # model.fit(train)\n",
    "    # save_model(model, 'models/')\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile ../src/models/predict_model.py\n",
    "\n",
    "# imports\n",
    "# helper functions go here\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that read files from data/processed,\n",
    "    calculates descriptive statistics for the population, and plots charts\n",
    "    that visualize interesting relationships between features.\n",
    "    \"\"\"\n",
    "    # test_X, test_y = load_test_data('data/processed')\n",
    "    # trained_model = load_model('models/')\n",
    "    # predictions = trained_model.predict(test_X)\n",
    "    # metrics = evaluate(test_y, predictions)\n",
    "    # save_metrics('reports/')\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write down any thoughts you may have about working with these algorithms on this data. What other ideas do you want to try out as you iterate on this pipeline?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret the Model\n",
    "\n",
    "_Write up the things you learned, and how well your model performed. Be sure address the model's strengths and weaknesses. What types of data does it handle well? What types of observations tend to give it a hard time? What future work would you or someone reading this might want to do, building on the lessons learned and tools developed in this project?_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
